{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formula 1 - Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import regex as re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Individual Race Information from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps taken were:\n",
    "##### Ergast API:\n",
    "\n",
    "    - Use json to extract information from 2014 onwards from Ergast API, containing F1 data from 1950 to present.\n",
    "    - All scraped information stored in a dictionary of lists and loaded into a dataframe.\n",
    "\n",
    "##### Processing:\n",
    "\n",
    "    - Alter 'race_name' variable by custom function for using in further scraping from URLs via format strings.\n",
    "    - Create further functions for cycling through each race's URL and extracting weather and distance information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = defaultdict(list)\n",
    "\n",
    "for year in list(range(2014, (datetime.datetime.now().date().year + 1))):\n",
    "    \n",
    "    url = f'https://ergast.com/api/f1/{year}.json'\n",
    "    r = requests.get(url)\n",
    "    json = r.json()\n",
    "\n",
    "    for item in json['MRData']['RaceTable']['Races']:\n",
    "        try:\n",
    "            races['season'].append(int(item['season']))\n",
    "        except:\n",
    "            races['season'].append(None)\n",
    "\n",
    "        try:\n",
    "            races['round'].append(int(item['round']))\n",
    "        except:\n",
    "            races['round'].append(None)\n",
    "\n",
    "        try:\n",
    "            races['race_name'].append(item['raceName'])\n",
    "        except:\n",
    "            races['race_name'].append(None)\n",
    "            \n",
    "        try:\n",
    "            races['circuitId'].append(item['Circuit']['circuitId'])\n",
    "        except:\n",
    "            races['circuitId'].append(None)\n",
    "            \n",
    "        try:\n",
    "            races['lat'].append(float(item['Circuit']['Location']['lat']))\n",
    "        except:\n",
    "            races['lat'].append(None)\n",
    "\n",
    "        try:\n",
    "            races['long'].append(float(item['Circuit']['Location']['long']))\n",
    "        except:\n",
    "            races['long'].append(None)\n",
    "\n",
    "        try:\n",
    "            races['country'].append(item['Circuit']['Location']['country'])\n",
    "        except:\n",
    "            races['country'].append(None)\n",
    "\n",
    "        try:\n",
    "            races['date'].append(item['date'])\n",
    "        except:\n",
    "            races['date'].append(None)\n",
    "\n",
    "        try:\n",
    "            races['url'].append(item['url'])\n",
    "        except:\n",
    "            races['url'].append(None)\n",
    "        \n",
    "races = pd.DataFrame(races)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['season', 'round', 'race_name', 'circuitId', 'lat', 'long', 'country',\n",
       "       'date', 'url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "races.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the Data into the Required Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def race_name(name):\n",
    "    split_name = name.split()\n",
    "    new_name = ('-'.join(split_name[:-2])).lower()\n",
    "    if new_name=='mexico-city':\n",
    "        return 'mexican'\n",
    "    if new_name=='united-states':\n",
    "        return 'us'\n",
    "    return new_name\n",
    "\n",
    "races.race_name = races.race_name.apply(race_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abu-dhabi'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_name('Abu Dhabi Grand Prix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['australian', 'malaysian', 'bahrain', 'chinese', 'spanish',\n",
       "       'monaco', 'canadian', 'austrian', 'british', 'german', 'hungarian',\n",
       "       'belgian', 'italian', 'singapore', 'japanese', 'russian', 'us',\n",
       "       'brazilian', 'abu-dhabi', 'mexican', 'european', 'azerbaijan',\n",
       "       'french', 'styrian', '70th-anniversary', 'tuscan', 'eifel',\n",
       "       'portuguese', 'emilia-romagna', 'turkish', 'sakhir', 'dutch',\n",
       "       'são-paulo', 'qatar', 'saudi-arabian', 'miami', 'las-vegas'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "races.race_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "races.season.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weather(cols):\n",
    "    for col in cols:\n",
    "        try:\n",
    "            if 'Weather' in str(col.find('th', attrs={'scope':'row'})):\n",
    "                return col.find('td').text.strip('\\n')\n",
    "        except:\n",
    "            return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_distance(cols):\n",
    "    for col in cols:\n",
    "        try:\n",
    "            if 'Distance' in str(col.find('th', attrs={'scope':'row'})):\n",
    "                return col.find('td').text.split(', ')[1].split('km')[0].strip(' ')\n",
    "        except:\n",
    "            return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather(url):\n",
    "    try:\n",
    "        result = requests.get(url)\n",
    "        soup = BeautifulSoup(result.text, 'html.parser')\n",
    "        table = soup.find('table', attrs={'class':'infobox'})\n",
    "        cols = table.find_all('tr')\n",
    "        weather = extract_weather(cols)\n",
    "        return weather\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(url):\n",
    "    try:\n",
    "        result = requests.get(url)\n",
    "        soup = BeautifulSoup(result.text, 'html.parser')\n",
    "        table = soup.find('table', attrs={'class':'infobox'})\n",
    "        cols = table.find_all('tr')\n",
    "        distance = extract_distance(cols)\n",
    "        return distance\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "races['weather'] = races.url.apply(weather)\n",
    "races['distance'] = races.url.apply(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Fansite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To fill gaps in the weather data, data was taken from f1-fansite.com. \n",
    "#### Steps taken were:\n",
    "##### Scraping:\n",
    "\n",
    "    - Use selenium to initiate an undetected chromedriver instance to circumvent cloud flare protection.\n",
    "    - Iterate through the different url structures used for races during the prescribed years.\n",
    "    - Use BeautifulSoup to search through the results and identify the location of the Weather Conditions info.\n",
    "    - Many commonly labelled sections, so the list of html objects was converted into string format.\n",
    "        - The strings were then iterated through to locate an instance containing the appropriate data.\n",
    "    - All unformatted output was then stored in the 'scraped_all' dict of lists, which was converted to a DF.\n",
    "\n",
    "##### Processing:\n",
    "\n",
    "    - Rows containing null values were then dropped and a process of raw data refinement ensued.\n",
    "    - Refinement included:\n",
    "            - String-section replacement.\n",
    "            - Regular expression to locate temperature ranges and take averages.\n",
    "    - Since the scraping took some time, the weather data was stored in csv format in the notebook directory.\n",
    "    - This weather was then combined with previously extracted weather data using the races dataframe's urls.\n",
    "        - Combination was completed via string concatenation.\n",
    "        - Multiple occurences of certain words is later dealt with by Binarised Count Vectorisation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!--     - First search through each string typed section for the word 'Weather'.\n",
    "    - Then search through the section with regular expression to separate out terms. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_fan_urls = ['https://www.f1-fansite.com/f1-result/race-results-{}-{}-f1-grand-prix/',\n",
    "               'https://www.f1-fansite.com/f1-result/race-results-{}-{}-f1-gp/',\n",
    "               'https://www.f1-fansite.com/f1-result/race-result-{}-{}-f1-gp/',\n",
    "               'https://www.f1-fansite.com/f1-result/{}-{}-grand-prix-race-results/',\n",
    "               'https://www.f1-fansite.com/f1-result/{}-{}-grand-prix-results/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for use in scraping\n",
    "\n",
    "race_dps = []\n",
    "\n",
    "for years in np.array(races.season.unique()):\n",
    "    race_dps.extend([(years, race_name) if race_name != '70th-anniversary' \n",
    "                     else (race_name.split('-')[0], race_name.split('-')[1], years)\n",
    "                     for race_name in \n",
    "                     list(races[races.season == years]['race_name'])])\n",
    "    race_dps.extend([(years, 'usa') for race_name in\n",
    "                 list(races[races.season == years]['race_name']) if race_name=='us'])\n",
    "    race_dps.extend([(years, 'mexico') for race_name in\n",
    "                 list(races[races.season == years]['race_name']) if race_name=='mexican'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'undetected_chromedriver'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mundetected_chromedriver\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01muc\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      4\u001b[0m scraped_all \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'undetected_chromedriver'"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver\n",
    "\n",
    "scraped_all = defaultdict(list)\n",
    "failed_all = []\n",
    "\n",
    "\n",
    "for race in race_dps:\n",
    "    found_weather = False\n",
    "    found_page = False\n",
    "    options = webdriver.ChromeOptions() \n",
    "    options.add_argument(\"start-maximized\")\n",
    "    for url in f1_fan_urls:\n",
    "        driver = uc.Chrome(options=options)\n",
    "        try:\n",
    "            driver.get(url.format(race[0], race[1]))\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            driver.quit()\n",
    "            if soup.find('body').get('class')[0] != 'error404':\n",
    "                found_page = True\n",
    "                break\n",
    "            else:\n",
    "                snippet = url.format(race[0], race[1]).strip('/').split('/')[-1]\n",
    "        except:\n",
    "            pass\n",
    "    if found_page == False:\n",
    "        if race[0] == '70th':\n",
    "            scraped_all['season'].append(race[2])\n",
    "            scraped_all['race_name'].append('-'.join([race[0], race[1]]))\n",
    "            scraped_all['weather'].append(np.nan)\n",
    "        else:\n",
    "            print(f'page not found for {race}')\n",
    "            scraped_all['season'].append(race[0])\n",
    "            scraped_all['race_name'].append(race[1])\n",
    "            scraped_all['weather'].append(np.nan)\n",
    "        continue\n",
    "\n",
    "    sections = soup.find_all('p')\n",
    "\n",
    "    for section in sections:\n",
    "        if 'Weather' in str(section):\n",
    "            found_weather = True\n",
    "            w_section = str(section)\n",
    "            cats = w_section.split('br')\n",
    "\n",
    "            for cat in cats:\n",
    "                if 'Weather' in cat:\n",
    "                    print(f\"data found for {race}\")\n",
    "                    if race[0] == '70th':\n",
    "                        scraped_all['season'].append(race[2])\n",
    "                        scraped_all['race_name'].append('-'.join([race[0], race[1]]))\n",
    "                        scraped_all['weather'].append(cat)\n",
    "                        break\n",
    "                    \n",
    "                    else:\n",
    "                        scraped_all['season'].append(race[0])\n",
    "                        scraped_all['race_name'].append(race[1])\n",
    "                        scraped_all['weather'].append(cat)\n",
    "    \n",
    "    if found_weather == False:\n",
    "        if race[0] == '70th':\n",
    "            scraped_all['season'].append(race[2])\n",
    "            scraped_all['race_name'].append('-'.join([race[0], race[1]]))\n",
    "            scraped_all['weather'].append(np.nan)\n",
    "        else:\n",
    "            scraped_all['season'].append(race[0])\n",
    "            scraped_all['race_name'].append(race[1])\n",
    "            scraped_all['weather'].append(np.nan)\n",
    "\n",
    "\n",
    "f1_fan_weather = pd.DataFrame(scraped_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(race_dps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_fan_weather[f1_fan_weather.weather.isnull()==True]\n",
    "f1_fan_complete = f1_fan_weather[f1_fan_weather.weather.isnull()==False]\n",
    "f1_fan_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = f1_fan_weather.copy()\n",
    "weather_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_filter(weather):\n",
    "    weather = weather.lower().replace('Weather:', '').replace('\\xa0', '').replace('<p> ', '').replace('<', '').replace('º', '°').replace('&amp;', '').replace(',', '').replace('/>', '').replace('/p>', '') \\\n",
    "    .replace('p>', '').replace('/a>', '').replace('☁', 'clouds').replace('☂', 'rain') \\\n",
    "    .replace('9.4.5', '9.4-9.5').replace('/', ' ').replace('dryovercast', 'dry overcast') \\\n",
    "    .replace('dryclouded', 'dry clouded').replace('drysunny', 'dry sunny') \\\n",
    "    .replace('\\xa0', '').replace('overcast22°c', 'overcast 22°c').replace('239', '23.9') \\\n",
    "    .replace('296', '29.6').replace('26.°c', '26°c').replace('22.3-24', '22.3-24.0') \\\n",
    "    .replace('20.4-°c', '20.4°c').replace('24.°c', '24°c').replace('clear26°c', 'clear 26°c') \\\n",
    "    .replace('dryclear', 'dry clear').replace('20.0', '20').replace('34.0', '34') \\\n",
    "    .replace('21.0', '21').replace('and', '').strip(' ')\n",
    "    \n",
    "    return weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_filter(weather):\n",
    "    if '-' in weather:\n",
    "        nums = re.findall(r'[0-9]+[.][0-9]', weather)\n",
    "        try:\n",
    "            mean = str((eval(nums[0])+eval(nums[1]))/2)\n",
    "            for num in nums:\n",
    "                weather = weather.replace('-'.join(nums), mean)\n",
    "            try:\n",
    "                wrong_format = re.findall(r'[0-9]+[\\.\\,]?[0-9]*', weather)[0]\n",
    "                new_format = wrong_format.replace(',', '.')\n",
    "                weather = weather.replace(wrong_format, new_format)\n",
    "            except:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "    return weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_extract(sub_soup):\n",
    "    try:\n",
    "        if '</i>' in sub_soup:\n",
    "            weath = sub_soup.split('</i>')[1:]\n",
    "            if len(weath)==1:\n",
    "                weather = weath[0].strip(' ').strip('<').replace('\\xa0', '') \\\n",
    "                .replace('i class=\"fa fa-thermometer-half\">', '')\n",
    "            else:\n",
    "                weather = (' '.join(weath)).replace('i class=\"fa fa-thermometer-half\">', '').replace('<', '')\n",
    "        else:\n",
    "            weather = sub_soup.replace('<p>Weather: ', '') \\\n",
    "            .replace('<', '')('i class=\"fa fa-thermometer-half\">', '').replace('<', '')\n",
    "    except:\n",
    "        weather = sub_soup.replace('Weather:', '').replace('\\xa0', '').replace('<p> ', '').replace('<', '')\n",
    "    \n",
    "    weather = text_filter(weather)\n",
    "    weather = range_filter(weather)\n",
    "    \n",
    "    return weather\n",
    "\n",
    "\n",
    "\n",
    "weather_df.weather = weather_df.weather.apply(weather_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.weather.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_df.to_csv('./CSV/weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv('./CSV/weather.csv').drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df[weather_df.race_name.str.contains('anniversary')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def race_name_paralleliser(race):\n",
    "    if race == 'usa':\n",
    "        return 'us'\n",
    "    elif race == 'mexico':\n",
    "        return 'mexican'\n",
    "    else:\n",
    "        return race\n",
    "    \n",
    "    \n",
    "    \n",
    "weather_df.race_name = weather_df.race_name.apply(race_name_paralleliser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_citations(data):\n",
    "    try:\n",
    "        citations = re.findall(r'[\\[][0-9][\\]]', data)\n",
    "        data = data.replace(citations[0], '')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_average(data):\n",
    "    try:\n",
    "        nums = re.findall(r'[0-9]+\\sto\\s[0-9]+', data)\n",
    "        for num in nums:\n",
    "            nums_sep = num.split(' to ')\n",
    "            average = (eval(nums_sep[0])+eval(nums_sep[1]))/2\n",
    "            data = data.replace(str(num), str(average))\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        nums = re.findall(r'[0-9]+[–][0-9]+', data)\n",
    "        for num in nums:\n",
    "            nums_sep = num.split('–')\n",
    "            average = (eval(nums_sep[0])+eval(nums_sep[1]))/2\n",
    "            data = data.replace(str(num), str(average))\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rogue_endings(data):\n",
    "    try:\n",
    "        rogue_s = re.findall(r'temperature[\\s]+[s]\\s', data)\n",
    "        for s in rogue_s:\n",
    "            data = data.replace(s, 'temperatures ')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        rogue_ing = re.findall(r'[\\s]+ing[\\s\\.\\,\\:\\;]+', data)\n",
    "        for ing in rogue_ing:\n",
    "            data = data.replace(ing, 'ing ')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def race_weather_extract(data):\n",
    "    \n",
    "    if (data == 'None') or (data == np.nan) or (data == 'nan'):\n",
    "        return ''\n",
    "    \n",
    "    try:\n",
    "        data = data.lower()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    data = str(data).replace('\\xa0', '').replace('sunny', 'sunny ') \\\n",
    "    .replace('temperature', 'temperature ').replace(';', ' ').replace('(', ' (') \\\n",
    "    .replace('cloudy', 'cloudy ').replace('clear', 'clear ').replace('later', 'later ') \\\n",
    "    .replace('dry', 'dry ').replace('times', 'times ').replace(':', ' ')\n",
    "    \n",
    "    data = remove_citations(data)\n",
    "    data = range_average(data)\n",
    "    data = remove_rogue_endings(data)    \n",
    "    \n",
    "    return data + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races.weather = races.weather.apply(race_weather_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# races.weather.apply(race_weather_extract).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "races_plus_all_weather = pd.merge(races, weather_df, on=['race_name', 'season'], how='outer')\n",
    "races_plus_all_weather.weather_y.fillna('', inplace=True)\n",
    "races_plus_all_weather['weather'] = races_plus_all_weather.weather_x + races_plus_all_weather.weather_y\n",
    "races_plus_all_weather.drop(['weather_x', 'weather_y', 'url'], axis=1, inplace=True)\n",
    "races_plus_all_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Results from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps taken were:\n",
    "##### Ergast API:\n",
    "\n",
    "    - Use json to extract the information from the Ergast API, containing F1 data from 1950 to present.\n",
    "    - All scraped information stored in a dictionary of lists and loaded into a dataframe.\n",
    "\n",
    "##### Processing:\n",
    "\n",
    "    - Overall race time for the drivers in millions of milliseconds.\n",
    "        - For the purpose of preliminary scaling, time variable divided by 1000 to convert to seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = []\n",
    "for year in np.array(races.season.unique()):\n",
    "    rounds.append([year, list(races[races.season == year]['round'])])\n",
    "\n",
    "# query API\n",
    "    \n",
    "results = defaultdict(list)\n",
    "\n",
    "for season in rounds:\n",
    "    for race in season[1]:\n",
    "        try:\n",
    "            url = f'https://ergast.com/api/f1/{season[0]}/{race}/results.json'\n",
    "            r = requests.get(url)\n",
    "            json = r.json()\n",
    "\n",
    "            item = json['MRData']['RaceTable']['Races'][0]\n",
    "            \n",
    "            for j in range(len(item['Results'])):\n",
    "                try:\n",
    "                    results['season'].append(int(item['season']))\n",
    "                except:\n",
    "                    results['season'].append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    results['round'].append(int(item['round']))\n",
    "                except:\n",
    "                    results['round'].append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    results['circuitId'].append(item['Circuit']['circuitId'])\n",
    "                except:\n",
    "                    results['circuitId'].append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    results['driverId'].append(item['Results'][j]['Driver']['driverId'])\n",
    "                except:\n",
    "                    results['driverId'].append(np.nan)\n",
    "                \n",
    "                try:\n",
    "                    results['finish_position'].append(int(item['Results'][j]['position']))\n",
    "                except:\n",
    "                    results['finish_position'].append(np.nan)    \n",
    "\n",
    "                try:\n",
    "                    results['date_of_birth'].append(item['Results'][j]['Driver']\n",
    "                                                    ['dateOfBirth'])\n",
    "                except:\n",
    "                    results['date_of_birth'].append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    results['nationality'].append(item['Results'][j]['Driver']\n",
    "                                                  ['nationality'])\n",
    "                except:\n",
    "                    results['nationality'].append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    results['constructor'].append(item['Results'][j]['Constructor']\n",
    "                                                  ['constructorId'])\n",
    "                except:\n",
    "                    results['constructor'].append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    results['grid'].append(int(item['Results'][j]['grid']))\n",
    "                except:\n",
    "                    results['grid'].append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    results['time'].append(int(item['Results'][j]['Time']['millis']))\n",
    "                except:\n",
    "                    results['time'].append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    results['status'].append(item['Results'][j]['status'])\n",
    "                except:\n",
    "                    results['status'].append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    results['points'].append(int(item['Results'][j]['points']))\n",
    "                except:\n",
    "                    results['points'].append(np.nan)\n",
    "\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.time = results.time/1000\n",
    "results[results.season==2014].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Qualifying Data from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps taken were:\n",
    "##### Ergast API:\n",
    "\n",
    "    - Use json to extract the information from the Ergast API, containing F1 data from 1950 to present.\n",
    "    - All scraped information stored in a dictionary of lists and loaded into a dataframe.\n",
    "\n",
    "##### Processing:\n",
    "\n",
    "    - Qualifying times reformatted to seconds and milliseconds using qual_time_formatter function.\n",
    "    - Average and Best qualifying times then produced for each driver for each race.\n",
    "        - This removes the issue of non-appearances in Q2 and Q3 for drivers qualifying below the threshold.\n",
    "    - Q1, Q2 & Q3 then dropped from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = []\n",
    "for year in np.array(races.season.unique()):\n",
    "    rounds.append([year, list(races[races.season == year]['round'])])\n",
    "\n",
    "# query API\n",
    "    \n",
    "qualis = defaultdict(list)\n",
    "\n",
    "for season in rounds:\n",
    "    for race in season[1]:\n",
    "        try:\n",
    "            url = f'https://ergast.com/api/f1/{season[0]}/{race}/qualifying.json'\n",
    "            r = requests.get(url)\n",
    "            json = r.json()\n",
    "\n",
    "            item = json['MRData']['RaceTable']['Races'][0]\n",
    "            for j in range(len(item['QualifyingResults'])):\n",
    "                try:\n",
    "                    qualis['season'].append(int(item['season']))\n",
    "                except:\n",
    "                    qualis['season'].append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    qualis['round'].append(int(item['round']))\n",
    "                except:\n",
    "                    qualis['round'].append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    qualis['circuitId'].append(item['Circuit']['circuitId'])\n",
    "                except:\n",
    "                    qualis['circuitId'].append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    qualis['driverId'].append(item['QualifyingResults'][j]['Driver']['driverId'])\n",
    "                except:\n",
    "                    qualis['driverId'].append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    qualis['qual_position'].append(int(item['QualifyingResults'][j]['position']))\n",
    "                except:\n",
    "                    qualis['qual_position'].append(np.nan)    \n",
    "\n",
    "                try:\n",
    "                    qualis['constructor'].append(item['QualifyingResults'][j]['Constructor']\n",
    "                                                  ['constructorId'])\n",
    "                except:\n",
    "                    qualis['constructor'].append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    qualis['q1'].append(str(item['QualifyingResults'][j]['Q1']))\n",
    "                except:\n",
    "                    qualis['q1'].append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    qualis['q2'].append(str(item['QualifyingResults'][j]['Q2']))\n",
    "                except:\n",
    "                    qualis['q2'].append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    qualis['q3'].append(str(item['QualifyingResults'][j]['Q3']))\n",
    "                except:\n",
    "                    qualis['q3'].append(np.nan)\n",
    "\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "qualifying = pd.DataFrame(qualis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifying.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qual_time_formatter(time):\n",
    "    try:\n",
    "        mins = eval(time[0])\n",
    "        if time[2] == '0':\n",
    "            secs = eval(time[3])\n",
    "        else:\n",
    "            secs = eval(time[2:4])\n",
    "        mils = float(time[-4:])\n",
    "        return((mins*60)+secs+mils)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "for qual in ['q1', 'q2', 'q3']:\n",
    "    qualifying[qual] = qualifying[qual].apply(qual_time_formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quali_best(a, b, c):\n",
    "    return min(a, b, c)\n",
    "\n",
    "qualifying['q_best'] = qualifying.apply(lambda x: quali_best(x[6], x[7], x[8]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quali_worst(a, b, c):\n",
    "    return max(a, b, c)\n",
    "\n",
    "qualifying['q_worst'] = qualifying.apply(lambda x: quali_worst(x[6], x[7], x[8]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quali_average(a, b, c):\n",
    "    try:\n",
    "        sums = []\n",
    "        for qual in [a, b, c]:\n",
    "            if qual > 0:\n",
    "                sums.append(qual)\n",
    "        return (sum(sums)/len(sums))\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "qualifying['q_mean'] = qualifying.apply(lambda x: quali_average(x[6], x[7], x[8]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all drivers make it out of q1 into q2, and then in to q3 - therefore for q2 and q3 there is some null data.\n",
    "\n",
    "For this reason I've taken the overall best, mean and worst qualifying times for each driver as an indicator of their capability at that particular track on that particular weekend.\n",
    "\n",
    "Since these are not strictly related, the three times should help identify consistency within qualifying for each driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifying.drop(['q1', 'q2', 'q3'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifying.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Driver Information from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps taken were:\n",
    "##### Ergast API:\n",
    "\n",
    "    - Use json to extract the information from the Ergast API, containing F1 data from 1950 to present.\n",
    "    - All scraped information stored in a dictionary of lists and loaded into a dataframe.\n",
    "\n",
    "##### Processing:\n",
    "\n",
    "    - No processing required at this stage.\n",
    "        - Further processing on this data during merge with other dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query API\n",
    "\n",
    "drivers = defaultdict(list)\n",
    "\n",
    "for year in results.season.unique():\n",
    "    url = f'http://ergast.com/api/f1/{year}/drivers.json'\n",
    "    r = requests.get(url)\n",
    "    json = r.json()\n",
    "    \n",
    "    try:\n",
    "        items = json['MRData']['DriverTable']['Drivers']\n",
    "        for j in range(len(items)):\n",
    "            \n",
    "            try:\n",
    "                drivers['driverId'].append(items[j]['driverId'])\n",
    "            except:\n",
    "                drivers['driverId'].append(np.nan)\n",
    "            \n",
    "            try:\n",
    "                forename = items[j]['givenName']\n",
    "                surname = items[j]['familyName']\n",
    "                \n",
    "                drivers['name'].append(forename + ' ' + surname)\n",
    "            except:\n",
    "                drivers['name'].append(np.nan)\n",
    "            \n",
    "            try:\n",
    "                drivers['nationality'].append(items[j]['nationality'])\n",
    "            except:\n",
    "                drivers['nationality'].append(np.nan)\n",
    "            \n",
    "            try:\n",
    "                drivers['code'].append(items[j]['code'])\n",
    "            except:\n",
    "                drivers['code'].append(np.nan)            \n",
    "            \n",
    "            try:\n",
    "                drivers['dateOfBirth'].append(items[j]['dateOfBirth'])\n",
    "            except:\n",
    "                drivers['dateOfBirth'].append(np.nan)\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "drivers = pd.DataFrame(drivers)\n",
    "drivers = drivers.drop_duplicates().reset_index(drop=True)\n",
    "drivers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers.nationality.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Circuit Information from API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps taken were:\n",
    "##### Ergast API:\n",
    "\n",
    "    - Use json to extract information contained within the Ergast API, containing F1 data from 1950 to present.\n",
    "    - All scraped information stored in a dictionary of lists and loaded into a dataframe.\n",
    "    \n",
    "##### Further Scraping from Wikipedia\n",
    "\n",
    "    - I found that wikipedia held all of the more in-depth information I required for the circuits, e.g.,\n",
    "        - Street or Race circuit.\n",
    "        - Clockwise/Counterclockwise (perhaps affecting the tyres of certain cars more for certain setups...)\n",
    "        - Individual lap length.\n",
    "        - Further information including name and location that were required for creating keys for merging.\n",
    "    - Functions were created for each of the above to process a BeautifulSoup object.\n",
    "\n",
    "##### Processing:\n",
    "\n",
    "    - No processing required at this stage.\n",
    "        - Further processing on this data during merge with other dataframes.\n",
    "        \n",
    "# still not finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query API\n",
    "\n",
    "circuits = defaultdict(list)\n",
    "\n",
    "for year in results.season.unique():\n",
    "    url = f'http://ergast.com/api/f1/{year}/circuits.json'\n",
    "    r = requests.get(url)\n",
    "    json = r.json()\n",
    "    \n",
    "    try:\n",
    "        items = json['MRData']['CircuitTable']['Circuits']\n",
    "        for j in range(len(items)):\n",
    "            \n",
    "            try:\n",
    "                circuits['circuitId'].append(items[j]['circuitId'])\n",
    "            except:\n",
    "                circuits['circuitId'].append(np.nan)\n",
    "            \n",
    "            try:\n",
    "                circuits['circuitName'].append(items[j]['circuitName'])\n",
    "            except:\n",
    "                circuits['circuitName'].append(np.nan)\n",
    "            \n",
    "            try:\n",
    "                circuits['lat'].append(items[j]['Location']['lat'])\n",
    "            except:\n",
    "                circuits['lat'].append(np.nan)\n",
    "\n",
    "            try:\n",
    "                circuits['long'].append(items[j]['Location']['long'])\n",
    "            except:\n",
    "                circuits['long'].append(np.nan)\n",
    "\n",
    "            try:\n",
    "                circuits['locality'].append(items[j]['Location']['locality'])\n",
    "            except:\n",
    "                circuits['locality'].append(np.nan)\n",
    "            \n",
    "            try:\n",
    "                circuits['country'].append(items[j]['Location']['country'])\n",
    "            except:\n",
    "                circuits['country'].append(np.nan)\n",
    "            \n",
    "            try:\n",
    "                circuits['url'].append(items[j]['url'])\n",
    "            except:\n",
    "                circuits['url'].append(np.nan)\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "circuits = pd.DataFrame(circuits)\n",
    "circuits = circuits.drop_duplicates().reset_index(drop=True)\n",
    "circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits.circuitId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(line, i):\n",
    "    try:\n",
    "        return line[i]\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped = defaultdict(list)\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_Formula_One_circuits'\n",
    "result = requests.get(url)\n",
    "soup = BeautifulSoup(result.text, 'html.parser')\n",
    "tables = soup.find_all('table', attrs={'class':'wikitable'})\n",
    "table = tables[1]\n",
    "body = table.find('tbody')\n",
    "for row in body.find_all('tr'):\n",
    "    row_info = []\n",
    "    line = [r.text.strip('\\n').strip('✔') for r in row.find_all('td') if r.text.strip('\\n')!='']\n",
    "\n",
    "    scraped['name'].append(extractor(line, 0))\n",
    "    track_type = extractor(line, 1)\n",
    "    try:\n",
    "        track_type.strip('circuit').strip(' ')\n",
    "    except:\n",
    "        pass\n",
    "    scraped['type'].append(track_type)\n",
    "    scraped['direction'].append(extractor(line, 2))\n",
    "    scraped['location'].append(extractor(line, 3))\n",
    "    scraped['length'].append(extractor(line, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_info = pd.DataFrame(scraped)\n",
    "circuits_info.drop([0], inplace=True)\n",
    "circuits_info.reset_index(drop=True, inplace=True)\n",
    "circuits_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "circuits_info.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_formatter(length):\n",
    "    return float(length.split('km')[0].strip(' ').strip('\\xa0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_info.length = circuits_info.length.apply(length_formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_info.length.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    None of the following circuits have not yet been completed since 2014 at the time of EDA, being scheduled for later in 2021.\n",
    "    \n",
    "    Circuit Zandvoort\n",
    "    Jeddah Street Circuit\n",
    "    Hard Rock Stadium Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits.circuitName.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def referencer(name):\n",
    "    name = name.lower()\n",
    "    \n",
    "    if 'adelaide' in name:\n",
    "        return 'adelaide'\n",
    "    elif 'diab' in name:\n",
    "        return 'ain-diab'\n",
    "    elif 'aintree' in name:\n",
    "        return 'aintree'\n",
    "    elif 'albert' in name:\n",
    "        return 'albert_park'\n",
    "    elif 'americas' in name:\n",
    "        return 'americas'\n",
    "    elif 'scandinavian' in name:\n",
    "        return 'anderstorp'\n",
    "    elif 'avus' in name:\n",
    "        return 'avus'\n",
    "    elif 'baku' in name:\n",
    "        return 'BAK'\n",
    "    elif 'bahrain' in name:\n",
    "        return 'bahrain'\n",
    "    elif 'boavista' in name:\n",
    "        return 'boavista'\n",
    "    elif 'brands hatch' in name:\n",
    "        return 'brands_hatch'\n",
    "    elif 'bremgarten' in name:\n",
    "        return 'bremgarten'\n",
    "    elif 'buddh' in name:\n",
    "        return 'buddh'\n",
    "    elif 'catalunya' in name:\n",
    "        return 'catalunya'\n",
    "    elif 'charade' in name:\n",
    "        return 'charade'\n",
    "    elif ('fair park' in name) or ('dallas' in name):\n",
    "        return 'dallas'\n",
    "    elif 'detroit' in name:\n",
    "        return 'detroit'\n",
    "    elif 'dijon' in name:\n",
    "        return 'dijon'\n",
    "    elif 'donington' in name:\n",
    "        return 'donington'\n",
    "    elif 'essarts' in name:\n",
    "        return 'essarts'\n",
    "    elif 'estoril' in name:\n",
    "        return 'estoril'\n",
    "    elif 'fuji' in name:\n",
    "        return 'fuji'\n",
    "    elif 'juan g' in name:\n",
    "        return 'galvez'\n",
    "    elif 'prince george' in name:\n",
    "        return 'george'\n",
    "    elif 'hanoi' in name:\n",
    "        return 'hanoi'\n",
    "    elif 'hockenheimring' in name:\n",
    "        return 'hockenheimring'\n",
    "    elif 'hungaroring' in name:\n",
    "        return 'hungaroring'\n",
    "    elif 'imperial' in name:\n",
    "        return 'port_imperial'\n",
    "    elif 'dino ferrari' in name:\n",
    "        return 'imola'\n",
    "    elif 'indianapolis' in name:\n",
    "        return 'indianapolis'\n",
    "    elif 'carlos pace' in name:\n",
    "        return 'interlagos'\n",
    "    elif 'istanbul' in name:\n",
    "        return 'istanbul'\n",
    "    elif 'jarama' in name:\n",
    "        return 'jarama'\n",
    "    elif 'piquet' in name:\n",
    "        return 'jacarepagua'\n",
    "    elif 'jeddah' in name:\n",
    "        return 'jeddah'\n",
    "    elif 'jerez' in name:\n",
    "        return 'jerez'\n",
    "    elif 'kyalami' in name:\n",
    "        return 'kyalami'\n",
    "    elif ('las vegas' in name) or ('caesars' in name):\n",
    "        return 'las_vegas'\n",
    "    elif ('le mans' in name) or ('bugatti' in name):\n",
    "        return 'lemans'\n",
    "    elif 'long beach' in name:\n",
    "        return 'long_beach'\n",
    "    elif 'magny' in name:\n",
    "        return 'magny_cours'\n",
    "    elif 'marina bay' in name:\n",
    "        return 'marina_bay'\n",
    "    elif 'monaco' in name:\n",
    "        return 'monaco'\n",
    "    elif 'monsanto' in name:\n",
    "        return 'monsanto'\n",
    "    elif 'montju' in name:\n",
    "        return 'montjuic'\n",
    "    elif 'monza' in name:\n",
    "        return 'monza'\n",
    "    elif 'mosport' in name:\n",
    "        return 'mosport'\n",
    "    elif 'mugello' in name:\n",
    "        return 'mugello'\n",
    "    elif 'nivelles' in name:\n",
    "        return 'nivelles'\n",
    "    elif 'nürburgring' in name:\n",
    "        return 'nurburgring'\n",
    "    elif 'okayama' in name:\n",
    "        return 'okayama'\n",
    "    elif 'a1' in name:\n",
    "        return 'osterreichring'\n",
    "    elif 'pedralbes' in name:\n",
    "        return 'pedralbes'\n",
    "    elif 'pescara' in name:\n",
    "        return 'pescara'\n",
    "    elif 'phoenix' in name:\n",
    "        return 'phoenix'\n",
    "    elif 'algarve' in name:\n",
    "        return 'portimao'\n",
    "    elif 'red bull' in name:\n",
    "        return 'red_bull_ring'\n",
    "    elif 'reims' in name:\n",
    "        return 'reims'\n",
    "    elif 'ricard' in name:\n",
    "        return 'ricard'\n",
    "    elif 'riverside' in name:\n",
    "        return 'riverside'\n",
    "    elif 'hermanos' in name:\n",
    "        return 'rodriguez'\n",
    "    elif 'sebring' in name:\n",
    "        return 'sebring'\n",
    "    elif 'sepang' in name:\n",
    "        return 'sepang'\n",
    "    elif 'shanghai' in name:\n",
    "        return 'shanghai'\n",
    "    elif 'silverstone' in name:\n",
    "        return 'silverstone'\n",
    "    elif 'sochi' in name:\n",
    "        return 'sochi'\n",
    "    elif 'francorchamps' in name:\n",
    "        return 'spa'\n",
    "    elif 'suzuka' in name:\n",
    "        return 'suzuka'\n",
    "    elif 'tremblant' in name:\n",
    "        return 'tremblant'\n",
    "    elif 'valencia' in name:\n",
    "        return 'valencia'\n",
    "    elif 'villeneuve' in name:\n",
    "        return 'villeneuve'\n",
    "    elif 'watkins' in name:\n",
    "        return 'watkins_glen'\n",
    "    elif 'yas marina' in name:\n",
    "        return 'yas_marina'\n",
    "    elif 'korea' in name:\n",
    "        return 'yeongam'\n",
    "    elif 'zandvoort' in name:\n",
    "        return 'zandvoort'\n",
    "    elif 'zeltweg' in name:\n",
    "        return 'zeltweg'\n",
    "    elif 'zolder' in name:\n",
    "        return 'zolder'\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_info['circuitId'] = circuits_info.name.apply(referencer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_info.circuitId.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_info[circuits_info.circuitId.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Hard Rock Stadium is a circuit that has been newly designated for the 2022 season and therefore can be dropped from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_info.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_info.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The names and locations of tracks are contained within both circuits and circuits_info dataframes, and therefore the names and locations of tracks will be dropped from the circuits_info dataframe before joining on cicuitRef for continuity purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_info.drop(['name', 'location'], axis=1, inplace=True)\n",
    "\n",
    "circuits_complete = circuits.merge(circuits_info, on='circuitId', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_complete.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the Comparison Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructor Standings Data for 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query API\n",
    "\n",
    "constructor_standings = defaultdict(list)\n",
    "\n",
    "url = f'http://ergast.com/api/f1/2020/constructorStandings.json'\n",
    "r = requests.get(url)\n",
    "json = r.json()\n",
    "\n",
    "try:\n",
    "    items = json['MRData']['StandingsTable']['StandingsLists'][0]['ConstructorStandings']\n",
    "    for item in items:\n",
    "\n",
    "        try:\n",
    "            constructor_standings['position'].append(int(item['position']))\n",
    "        except:\n",
    "            constructor_standings['position'].append(np.nan)\n",
    "\n",
    "        try:\n",
    "            constructor_standings['constructor'].append(item['Constructor']['constructorId'])\n",
    "        except:\n",
    "            constructor_standings['constructor'].append(np.nan)\n",
    "\n",
    "        try:\n",
    "            constructor_standings['points'].append(int(item['points']))\n",
    "        except:\n",
    "            constructor_standings['points'].append(np.nan)\n",
    "\n",
    "except:\n",
    "    pass\n",
    "        \n",
    "\n",
    "constructor_standings = pd.DataFrame(constructor_standings)\n",
    "constructor_standings = constructor_standings.drop_duplicates().reset_index(drop=True)\n",
    "constructor_standings.to_csv('./CSV/constructor_standings.csv')\n",
    "constructor_standings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver Standings Data for 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query API\n",
    "\n",
    "driver_standings = defaultdict(list)\n",
    "\n",
    "url = f'http://ergast.com/api/f1/2020/driverStandings.json'\n",
    "r = requests.get(url)\n",
    "json = r.json()\n",
    "\n",
    "try:\n",
    "    items = json['MRData']['StandingsTable']['StandingsLists'][0]['DriverStandings']\n",
    "    for item in items:\n",
    "        \n",
    "        try:\n",
    "            driver_standings['driverId'].append(item['Driver']['driverId'])\n",
    "        except:\n",
    "            driver_standings['driverId'].append(np.nan)\n",
    "        \n",
    "        try:\n",
    "            driver_standings['position'].append(int(item['position']))\n",
    "        except:\n",
    "            driver_standings['position'].append(np.nan)\n",
    "        \n",
    "        try:\n",
    "            driver_standings['points'].append(int(item['points']))\n",
    "        except:\n",
    "            driver_standings['points'].append(np.nan)\n",
    "        \n",
    "except:\n",
    "    pass\n",
    "        \n",
    "\n",
    "driver_standings = pd.DataFrame(driver_standings)\n",
    "driver_standings.to_csv('./CSV/driver_standings.csv')\n",
    "driver_standings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_qual = pd.merge(results, qualifying, on=['circuitId', 'season', 'round', 'driverId', 'constructor'], how='outer')\n",
    "races_final = races_plus_all_weather.drop(['lat', 'long', 'country'], axis=1)\n",
    "race_res_qual = pd.merge(races_final, res_qual, on=['circuitId', 'season', 'round'], how='outer')\n",
    "race_res_qual.drop(['date_of_birth', 'nationality'], axis=1, inplace=True)\n",
    "driver_race_res_qual = pd.merge(race_res_qual, drivers, on='driverId', how='outer')\n",
    "merged = pd.merge(driver_race_res_qual, circuits_complete, on='circuitId', how='outer').drop(['url'], axis=1)\n",
    "\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('./CSV/merged_database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged = pd.read_csv('./CSV/merged_database.csv').drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.dateOfBirth = pd.to_datetime(merged.dateOfBirth)\n",
    "merged.date = pd.to_datetime(merged.date)\n",
    "merged['ageDuringRace'] = merged.apply(lambda x: x[4] - x[21], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many constructors' names have changed over the years due to things such as sponsorship and mergers, with minimal changes to the structure of the team. Therefore the names of those such as Aston Martin, which has changed names twice in the hybrid era since being bought by Lawrence Stroll from Force India and rebranding to Racing Point until the end of the 2020 season before teaming up with Aston Martin as sponsors.\n",
    "\n",
    "All names have been kept synonymous with the names as of the 2020 season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.constructor.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructor_combine(constructor):\n",
    "    if (constructor=='alpine') or (constructor=='lotus_f1'):\n",
    "        return 'renault'\n",
    "    if (constructor=='force_india') or (constructor=='aston_martin'):\n",
    "        return 'racing_point'\n",
    "    if constructor=='toro_rosso':\n",
    "        return 'alphatauri'\n",
    "    if constructor=='marussia':\n",
    "        return 'manor'\n",
    "    if constructor=='sauber':\n",
    "        return 'alfa'\n",
    "    return constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.constructor = merged.constructor.apply(constructor_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.constructor.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = merged[['season', 'round', 'race_name',\n",
    "                     'name', 'constructor', 'grid', 'qual_position',\n",
    "                     'q_best', 'q_worst', 'q_mean', 'ageDuringRace', 'circuitId',\n",
    "                     'locality', 'country', 'type', 'direction', 'length', \n",
    "                     'weather', 'time', 'finish_position', 'status', 'points']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = main_df.sort_values(by=['season', 'round', 'finish_position'])\n",
    "main_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "time_exerpt = main_df.time\n",
    "position_exerpt = main_df.finish_position\n",
    "minimum = []\n",
    "\n",
    "for i in range(len(position_exerpt)):\n",
    "    if position_exerpt[i]==1:\n",
    "        minimum.append(time_exerpt[i])\n",
    "    else:\n",
    "        minimum.append(np.nan)\n",
    "\n",
    "main_df['min'] = minimum\n",
    "main_df['min'].ffill(inplace=True)\n",
    "main_df['split_times'] = main_df['time'] - main_df['min']\n",
    "main_df.split_times.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_df.split_times.ffill(inplace=True)\n",
    "\n",
    "def split_compute(split, status):\n",
    "    try:\n",
    "        if 'Laps' in status:\n",
    "            if type(eval(status[1])) == int:\n",
    "                return split*int(status[1])\n",
    "        else:\n",
    "            return split\n",
    "    except:\n",
    "        return split\n",
    "\n",
    "\n",
    "\n",
    "new_splits = []\n",
    "\n",
    "for i in range(main_df.shape[0]):\n",
    "    new_splits.append(split_compute(main_df.split_times[i], main_df.status[i]))\n",
    "\n",
    "main_df['filled_splits'] = new_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.drop(['time', 'min', 'status', 'split_times'], axis=1, inplace=True)\n",
    "main_df.dropna(inplace=True)\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.to_csv('./CSV/main_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tableau CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age vs. Pointscoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_points = merged[['name', 'ageDuringRace', 'points']].copy()\n",
    "age_points.isnull().sum()\n",
    "age_points.dropna(inplace=True)\n",
    "age_points.ageDuringRace = age_points.ageDuringRace.dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_bracket(age):\n",
    "    return age//365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_points.ageDuringRace = age_points.ageDuringRace.apply(age_bracket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_points.to_csv('./CSV/age_points.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_points[age_points.ageDuringRace==38]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home Nation Pointscoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.nationality.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.country.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def home_nation(nat):\n",
    "    if nat == 'German':\n",
    "        return 'Germany'\n",
    "    elif nat == 'British':\n",
    "        return 'UK'\n",
    "    elif nat == 'Spanish':\n",
    "        return 'Spain'\n",
    "    elif nat == 'French':\n",
    "        return 'France'\n",
    "    elif nat == 'Russian':\n",
    "        return 'Russia'\n",
    "    elif nat == 'Mexican':\n",
    "        return 'Mexico'\n",
    "    elif nat == 'Brazilian':\n",
    "        return 'Brazil'\n",
    "    elif nat == 'Japanese':\n",
    "        return 'Japan'\n",
    "    elif nat == 'Australian':\n",
    "        return 'Australia'\n",
    "    elif nat == 'Dutch':\n",
    "        return 'Austria'\n",
    "    elif nat == 'Belgian':\n",
    "        return 'Belgium'\n",
    "    elif nat == 'Italian':\n",
    "        return 'Italy'\n",
    "    elif nat == 'Canadian':\n",
    "        return 'Canada'\n",
    "    elif nat == 'Monegasque':\n",
    "        return 'Monaco'\n",
    "    elif nat == 'American':\n",
    "        return 'USA'\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver_country_filter(country):\n",
    "    driver_nations = ['Germany', 'Denmark', 'UK', 'Spain', 'Finland', 'France',\n",
    "                      'Russia', 'Mexico', 'Venezuela', 'Sweden', 'Brazil',\n",
    "                      'Japan', 'Australia', 'Austria', 'Indonesia', 'Belgium',\n",
    "                      'Italy', 'Canada', 'New Zealand', 'Monaco', 'Thailand',\n",
    "                      'Poland', 'USA']\n",
    "    if country in driver_nations:\n",
    "        return country\n",
    "    else:\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_races = merged[['country', 'nationality', 'points']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_races.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_races['scored'] = (home_races['points']>0)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_races.drop(['points'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_races['nat_country'] = home_races.nationality.apply(home_nation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_races.country = home_races.country.apply(driver_country_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_races.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_races.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = home_races.sort_values(by='scored', ascending=False).groupby(['country', 'nationality']).agg('mean')#.sort_values(by='scored', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.sort_values(by=['country', 'scored'], ascending=[True, False]).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_races.to_csv('./CSV/home_races.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver & Constructor Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_fault = ['Finished', '+2 Laps', '+1 Lap','+8 Laps', '+3 Laps', '+4 Laps', '+5 Laps','+6 Laps']\n",
    " \n",
    "driver_fault = ['Retired', 'Withdrew', 'Collision', 'Accident', 'Disqualified', 'Damage',\n",
    "                'Spun off', 'Collision damage', 'Puncture', 'Rear wing', 'Tyre', 'Front wing',\n",
    "                'Excluded', 'Illness']\n",
    "\n",
    "car_fault = ['Suspension', 'Wheel', 'Vibrations', 'Engine', 'ERS',\n",
    "              'Power loss', 'Water leak',  'Oil pressure', 'Hydraulics',\n",
    "              'Steering', 'Power Unit', 'Brakes', 'Mechanical', 'Turbo',\n",
    "              'Battery', 'Electrical', 'Gearbox', 'Wheel nut', 'Technical',\n",
    "              'Fuel system', 'Clutch', 'Out of fuel', 'Driveshaft',\n",
    "              'Transmission', 'Fuel pressure', 'Exhaust','Oil leak', \n",
    "              'Electronics', 'Drivetrain','Overheating',  'Water pressure',\n",
    "              'Radiator','Debris', 'Throttle', 'Spark plugs', 'Brake duct', 'Seat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status_fault(status, no=no_fault, driver=driver_fault):\n",
    "    if status in no:\n",
    "        return 'finish'\n",
    "    elif status in driver:\n",
    "        return 'driver'\n",
    "    else:\n",
    "        return 'car'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_issues = merged[['season', 'name', 'constructor', 'status']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_issues['fault'] = status_issues.status.apply(status_fault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_issues.drop(['status'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_issues = status_issues[['season', 'name', 'fault']].copy()\n",
    "driver_issues = driver_issues[driver_issues.fault!='car']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver_issues.fault = (driver_issues.fault=='finish')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver_issues.to_csv('./CSV/driver_issues.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_issues.fault = (driver_issues.fault=='driver')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_issues.to_csv('./CSV/driver_issues_faults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_issues_grouped = driver_issues.groupby(['season', 'name']).agg('mean').sort_values(by=['season', 'fault'], ascending=False)[6:]\n",
    "driver_issues_grouped.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructor_issues = status_issues[['season', 'constructor', 'fault']].copy()\n",
    "constructor_issues = constructor_issues[constructor_issues.fault!='driver']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructor_issues.fault = (constructor_issues.fault=='car')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructor_issues.to_csv('./CSV/constructor_issues_faults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructor_issues.fault.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructor_issues.groupby('constructor').agg('mean').sort_values(by='fault', ascending=False).head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
